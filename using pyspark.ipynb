{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages to establish spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensor_Locationsdf = spark.read.csv(path='Pedestrian_Counting_System_-_Sensor_Locations.csv',\n",
    "                        sep=',',\n",
    "                        encoding='UTF-8',\n",
    "                        comment=None,\n",
    "                        header=True, \n",
    "                        inferSchema=True)\n",
    "\n",
    "counts_per_hourdf = spark.read.csv(path='Pedestrian_Counting_System___2009_to_Present__counts_per_hour_.csv',\n",
    "                        sep=',',\n",
    "                        encoding='UTF-8',\n",
    "                        comment=None,\n",
    "                        header=True, \n",
    "                        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_per_hourdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           Date_Time|\n",
      "+--------------------+\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "|11/01/2019 05:00:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_per_hourdf.select('Date_Time').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_hourdf = counts_per_hourdf.withColumn(\"Date_Time\", \n",
    "                    to_timestamp(unix_timestamp(counts_per_hourdf.Date_Time, 'MM/dd/yyyy hh:mm:ss aa').cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_per_hourdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+\n",
      "|     ID|          Date_Time|Year|   Month|Mdate|   Day|Time|Sensor_ID|         Sensor_Name|Hourly_Counts|\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+\n",
      "|2887628|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       34|Flinders St-Spark La|          300|\n",
      "|2887629|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       39|        Alfred Place|          604|\n",
      "|2887630|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       37|     Lygon St (East)|          216|\n",
      "|2887631|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       40|Lonsdale St-Sprin...|          627|\n",
      "|2887632|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       36|     Queen St (West)|          774|\n",
      "|2887633|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       29|St Kilda Rd-Alexa...|          644|\n",
      "|2887634|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       42|Grattan St-Swanst...|          453|\n",
      "|2887635|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       43|Monash Rd-Swansto...|          387|\n",
      "|2887636|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       44|Tin Alley-Swansto...|           27|\n",
      "|2887637|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       35|           Southbank|         2691|\n",
      "|2887638|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       45|Little Collins St...|         2173|\n",
      "|2887639|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       46|       Pelham St (S)|          203|\n",
      "|2887640|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       47|Melbourne Central...|         2354|\n",
      "|2887641|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       48| QVM-Queen St (East)|          358|\n",
      "|2887642|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       49|QVM-Therry St (So...|          161|\n",
      "|2887643|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       50|Faraday St-Lygon ...|          502|\n",
      "|2887644|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       51|QVM-Franklin St (...|          159|\n",
      "|2887645|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       52|Elizabeth St-Lons...|          914|\n",
      "|2887646|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       54|Lincoln-Swanston(...|          276|\n",
      "|2887647|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       55|Elizabeth St-La T...|         2070|\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_per_hourdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Datetime to a Datekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+--------+\n",
      "|     ID|          Date_Time|Year|   Month|Mdate|   Day|Time|Sensor_ID|         Sensor_Name|Hourly_Counts| DateKey|\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+--------+\n",
      "|2887628|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       34|Flinders St-Spark La|          300|20191101|\n",
      "|2887629|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       39|        Alfred Place|          604|20191101|\n",
      "|2887630|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       37|     Lygon St (East)|          216|20191101|\n",
      "|2887631|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       40|Lonsdale St-Sprin...|          627|20191101|\n",
      "|2887632|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       36|     Queen St (West)|          774|20191101|\n",
      "|2887633|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       29|St Kilda Rd-Alexa...|          644|20191101|\n",
      "|2887634|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       42|Grattan St-Swanst...|          453|20191101|\n",
      "|2887635|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       43|Monash Rd-Swansto...|          387|20191101|\n",
      "|2887636|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       44|Tin Alley-Swansto...|           27|20191101|\n",
      "|2887637|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       35|           Southbank|         2691|20191101|\n",
      "|2887638|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       45|Little Collins St...|         2173|20191101|\n",
      "|2887639|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       46|       Pelham St (S)|          203|20191101|\n",
      "|2887640|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       47|Melbourne Central...|         2354|20191101|\n",
      "|2887641|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       48| QVM-Queen St (East)|          358|20191101|\n",
      "|2887642|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       49|QVM-Therry St (So...|          161|20191101|\n",
      "|2887643|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       50|Faraday St-Lygon ...|          502|20191101|\n",
      "|2887644|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       51|QVM-Franklin St (...|          159|20191101|\n",
      "|2887645|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       52|Elizabeth St-Lons...|          914|20191101|\n",
      "|2887646|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       54|Lincoln-Swanston(...|          276|20191101|\n",
      "|2887647|2019-11-01 17:00:00|2019|November|    1|Friday|  17|       55|Elizabeth St-La T...|         2070|20191101|\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+--------------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "counts_per_hourdf = counts_per_hourdf.withColumn(\"DateKey\", date_format(col(\"Date_Time\"), \"yyyyMMdd\"))\n",
    "counts_per_hourdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Dimensions and Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3963"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DimDateDF = counts_per_hourdf.select(\"DateKey\",\"Year\", \"Month\",\"Mdate\")\n",
    "DimDateDF = DimDateDF.dropDuplicates()\n",
    "DimDateDF.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3048367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FactPedCountPerSensorDF = counts_per_hourdf.select(\"ID\", \"DateKey\",\"Date_Time\", \"Sensor_ID\",\"Hourly_Counts\")\n",
    "FactPedCountPerSensorDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DimSensorsDF=Sensor_Locationsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- sensor_description: string (nullable = true)\n",
      " |-- sensor_name: string (nullable = true)\n",
      " |-- installation_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- direction_1: string (nullable = true)\n",
      " |-- direction_2: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DimSensorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load- Data into S3 in an appropriate format for future querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FactPedCountPerSensorDF.write.format(\"csv\").mode(\"overwrite\").save(\"FactPedCountPerSensor.csv\",header=True)\n",
    "DimSensorsDF.write.format(\"csv\").mode(\"overwrite\").save(\"DimSensors.csv\",header=True)\n",
    "DimDateDF.write.format(\"csv\").mode(\"overwrite\").save(\"DimDate.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying for future Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FactPedCountPerSensorDF.createGlobalTempView(\"PedCountPerSensor\")\n",
    "DimDateDF.createGlobalTempView(\"DimDate\")\n",
    "DimSensorsDF.createGlobalTempView(\"DimSensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FactPedCountPerSensorDF.createGlobalTempView(\"PedCountPerSensor\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "Top10LocPerDayDF = spark.sql('''\n",
    "          with Top10PerDay as (\n",
    "          SELECT \n",
    "          DateKey,\n",
    "          Sensor_ID,\n",
    "          SUM(Hourly_Counts) AS TotalCounts,\n",
    "          ROW_NUMBER() OVER(Partition by DateKey ORDER BY SUM(Hourly_Counts) DESC) AS Ranked\n",
    "          FROM global_temp.PedCountPerSensor\n",
    "          GROUP BY\n",
    "          DateKey,\n",
    "          Sensor_ID)\n",
    "          \n",
    "          SELECT \n",
    "          DateKey,\n",
    "          Que1.Sensor_ID,\n",
    "          sensor_description,\n",
    "          TotalCounts\n",
    "          FROM \n",
    "          Top10PerDay Que1 INNER JOIN global_temp.DimSensors DS ON Que1.Sensor_ID = DS.sensor_id \n",
    "          WHERE  \n",
    "          Ranked<=10\n",
    "          \n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----------+\n",
      "| DateKey|Sensor_ID|  sensor_description|TotalCounts|\n",
      "+--------+---------+--------------------+-----------+\n",
      "|20100117|        4|    Town Hall (West)|      27633|\n",
      "|20100117|        5|      Princes Bridge|      21484|\n",
      "|20100117|        2|Bourke Street Mal...|      19931|\n",
      "|20100117|        3|   Melbourne Central|      19100|\n",
      "|20100117|        6|Flinders Street S...|      15517|\n",
      "|20100117|        1|Bourke Street Mal...|      13202|\n",
      "|20100117|        7|      Birrarung Marr|      10111|\n",
      "|20100117|       15|       State Library|       9820|\n",
      "|20100117|       16|Australia on Collins|       8619|\n",
      "|20100117|       11|     Waterfront City|       6439|\n",
      "|20100315|        4|    Town Hall (West)|      32713|\n",
      "|20100315|        3|   Melbourne Central|      29207|\n",
      "|20100315|       13|   Flagstaff Station|      26510|\n",
      "|20100315|        6|Flinders Street S...|      26052|\n",
      "|20100315|        5|      Princes Bridge|      24416|\n",
      "|20100315|        2|Bourke Street Mal...|      24069|\n",
      "|20100315|       15|       State Library|      20500|\n",
      "|20100315|        1|Bourke Street Mal...|      19232|\n",
      "|20100315|       16|Australia on Collins|      18685|\n",
      "|20100315|       17|Collins Place (So...|      14443|\n",
      "+--------+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Top10LocPerDayDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top10LocPerMonthDF = spark.sql('''\n",
    "          with Top10PerMonth as (\n",
    "          SELECT \n",
    "          DD.Year,\n",
    "          DD.Month,\n",
    "          Sensor_ID,\n",
    "          SUM(Hourly_Counts) AS TotalCounts,\n",
    "          ROW_NUMBER() OVER(Partition by DD.Year,DD.Month ORDER BY SUM(Hourly_Counts) DESC) AS Ranked\n",
    "          FROM global_temp.PedCountPerSensor Cnt INNER JOIN \n",
    "          global_temp.DimDate DD on Cnt.DateKey = DD.DateKey\n",
    "          GROUP BY\n",
    "          DD.Year,\n",
    "          DD.Month,\n",
    "          Sensor_ID)\n",
    "          \n",
    "          SELECT \n",
    "          Year,\n",
    "          Month,\n",
    "          Que1.Sensor_ID,\n",
    "          sensor_description,\n",
    "          TotalCounts\n",
    "          FROM \n",
    "          Top10PerMonth Que1 INNER JOIN global_temp.DimSensors DS ON Que1.Sensor_ID = DS.sensor_id \n",
    "          WHERE  \n",
    "          Ranked<=10\n",
    "          \n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+--------------------+-----------+\n",
      "|Year|   Month|Sensor_ID|  sensor_description|TotalCounts|\n",
      "+----+--------+---------+--------------------+-----------+\n",
      "|2012|February|        4|    Town Hall (West)|    1136505|\n",
      "|2012|February|        3|   Melbourne Central|     797638|\n",
      "|2012|February|        6|Flinders Street S...|     796421|\n",
      "|2012|February|        2|Bourke Street Mal...|     795730|\n",
      "|2012|February|        1|Bourke Street Mal...|     693906|\n",
      "|2012|February|        5|      Princes Bridge|     629101|\n",
      "|2012|February|       13|   Flagstaff Station|     566731|\n",
      "|2012|February|       15|       State Library|     548209|\n",
      "|2012|February|       16|Australia on Collins|     501062|\n",
      "|2012|February|       17|Collins Place (So...|     350724|\n",
      "|2011|November|        4|    Town Hall (West)|    1080001|\n",
      "|2011|November|        2|Bourke Street Mal...|     927147|\n",
      "|2011|November|        6|Flinders Street S...|     827084|\n",
      "|2011|November|        3|   Melbourne Central|     796439|\n",
      "|2011|November|        5|      Princes Bridge|     596603|\n",
      "|2011|November|       13|   Flagstaff Station|     547839|\n",
      "|2011|November|       16|Australia on Collins|     535336|\n",
      "|2011|November|       15|       State Library|     438092|\n",
      "|2011|November|       17|Collins Place (So...|     366921|\n",
      "|2011|November|        9|Southern Cross St...|     281895|\n",
      "+----+--------+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Top10LocPerMonthDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above results can be send to database or s3 using glue "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
